# Introduction aux LLM

### Table des matières
1.  [Qu'est-ce qu'un LLM ?](#1--qu'est-ce-qu'un-llm?)
2.  [Historique et évolution](#2--historique-et-évolution)
3.  [Principaux types de modèles (Encoder-only, Decoder-only, etc.)](#3-principaux-types-de-modèles)
4.  [Cas d'usage courants](#4-cas-dusage-courants)

#### 1- Qu'est-ce qu'un LLM ?

#### 2- Historique et évolution

GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results [June 2018]

BERT, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!) [October 2018]

GPT-2, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns [February 2019]

T5, A multi-task focused implementation of the sequence-to-sequence Transformer architecture. [October 2019]

GPT-3, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called zero-shot learning) [May 2020]

InstructGPT, a version of GPT-3 that was trained to follow instructions better This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories: [January 2022]

Llama, a large language model that is able to generate text in a variety of languages. [January 2023]

Mistral, a 7-billion-parameter language model that outperforms Llama 2 13B across all evaluated benchmarks, leveraging grouped-query attention for faster inference and sliding window attention to handle sequences of arbitrary length. [March 2023]

Gemma 2, a family of lightweight, state-of-the-art open models ranging from 2B to 27B parameters that incorporate interleaved local-global attentions and group-query attention, with smaller models trained using knowledge distillation to deliver performance competitive with models 2-3 times larger. [May 2024]

SmolLM2, a state-of-the-art small language model (135 million to 1.7 billion parameters) that achieves impressive performance despite its compact size, and unlocking new possibilities for mobile and edge devices. [November 2024]


#### 3. Principaux types de modèles


#### 4. Cas d'usage courants
